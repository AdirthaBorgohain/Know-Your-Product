{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import nlpaug.augmenter.word as naw\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import TFDistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>review_title</th>\n",
       "      <th>review_text</th>\n",
       "      <th>rating</th>\n",
       "      <th>age_range</th>\n",
       "      <th>skin_type</th>\n",
       "      <th>skin_tone</th>\n",
       "      <th>eye_color</th>\n",
       "      <th>reviewer_username</th>\n",
       "      <th>tags</th>\n",
       "      <th>review_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P38217</td>\n",
       "      <td>Worth the money</td>\n",
       "      <td>Sometimes I stray from this cleanser, but I al...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>normal</td>\n",
       "      <td>light</td>\n",
       "      <td>NaN</td>\n",
       "      <td>katechatte</td>\n",
       "      <td>{foamy,exfoliating}</td>\n",
       "      <td>6611717f-2636-4756-bf36-66c81cc267a7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P38217</td>\n",
       "      <td>Great</td>\n",
       "      <td>I am a 41 year old African American woman with...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>combination</td>\n",
       "      <td>deep</td>\n",
       "      <td>NaN</td>\n",
       "      <td>snook41</td>\n",
       "      <td>{foamy}</td>\n",
       "      <td>e7d3307e-02ff-45a1-8fc3-6bd628bedd86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P38217</td>\n",
       "      <td>Great Product</td>\n",
       "      <td>I'm really enjoying this product. Received a s...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>combination</td>\n",
       "      <td>olive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wahinewarrior</td>\n",
       "      <td>{foamy,milky,exfoliating}</td>\n",
       "      <td>4188d728-fde6-4d06-984e-164cca2b8781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P38217</td>\n",
       "      <td>Nice, but not great for combination skin</td>\n",
       "      <td>I tried this cleanser at a friends house, and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>combination</td>\n",
       "      <td>fair</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jenlines22</td>\n",
       "      <td>{hydrating,creamy}</td>\n",
       "      <td>248c904c-6e30-4929-8228-87b03ad7a921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P38217</td>\n",
       "      <td>great moisturizer</td>\n",
       "      <td>leaves the skin feeling fresh and revived... j...</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dry</td>\n",
       "      <td>light</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jessea</td>\n",
       "      <td>{exfoliating}</td>\n",
       "      <td>654bdb99-9371-4440-a540-0dd2a73da339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product_id                              review_title  \\\n",
       "0     P38217                           Worth the money   \n",
       "1     P38217                                     Great   \n",
       "2     P38217                             Great Product   \n",
       "3     P38217  Nice, but not great for combination skin   \n",
       "4     P38217                         great moisturizer   \n",
       "\n",
       "                                         review_text  rating  age_range  \\\n",
       "0  Sometimes I stray from this cleanser, but I al...       5        NaN   \n",
       "1  I am a 41 year old African American woman with...       5        NaN   \n",
       "2  I'm really enjoying this product. Received a s...       5        NaN   \n",
       "3  I tried this cleanser at a friends house, and ...       3        NaN   \n",
       "4  leaves the skin feeling fresh and revived... j...       5        NaN   \n",
       "\n",
       "     skin_type skin_tone  eye_color reviewer_username  \\\n",
       "0       normal     light        NaN        katechatte   \n",
       "1  combination      deep        NaN           snook41   \n",
       "2  combination     olive        NaN     wahinewarrior   \n",
       "3  combination      fair        NaN        jenlines22   \n",
       "4          dry     light        NaN            jessea   \n",
       "\n",
       "                        tags                             review_id  \n",
       "0        {foamy,exfoliating}  6611717f-2636-4756-bf36-66c81cc267a7  \n",
       "1                    {foamy}  e7d3307e-02ff-45a1-8fc3-6bd628bedd86  \n",
       "2  {foamy,milky,exfoliating}  4188d728-fde6-4d06-984e-164cca2b8781  \n",
       "3         {hydrating,creamy}  248c904c-6e30-4929-8228-87b03ad7a921  \n",
       "4              {exfoliating}  654bdb99-9371-4440-a540-0dd2a73da339  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel('data/sephora_review (5 products).xlsx', sheet_name='sephora_review', engine='openpyxl')\n",
    "df = df[df['review_text'].notna()]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate sentiment labels from the data using the ratings given by each user. The reviews that have a rating of less than 3 are given negative sentiment labels and those having 3 or more than 3 stars are given positive sentiment labels. For training purposes, we map 'positive' and 'negative' labels to 1 and 0 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5234537ad5444a11809d454c69a46fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sometimes I stray from this cleanser, but I al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am a 41 year old African American woman with...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm really enjoying this product. Received a s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I tried this cleanser at a friends house, and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>leaves the skin feeling fresh and revived... j...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  sentiment\n",
       "0  Sometimes I stray from this cleanser, but I al...          1\n",
       "1  I am a 41 year old African American woman with...          1\n",
       "2  I'm really enjoying this product. Received a s...          1\n",
       "3  I tried this cleanser at a friends house, and ...          1\n",
       "4  leaves the skin feeling fresh and revived... j...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = df['rating'].swifter.apply(lambda score: \"positive\" if score >= 3 else \"negative\")\n",
    "df['sentiment'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n",
    "df = df[[\"review_text\", \"sentiment\"]]\n",
    "df.sample(frac=1).reset_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    891\n",
       "0    109\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the distribution of labels in the data, we see that there are far more positive reviews than negative reviews. During training, this might induce bias into the model. To counter this, we will apply a text augmentation method using which we will generate new texts from a given text which has the same semantics and meaning but will just have a different framing. We apply this text augmentation to only the negative reviewed texts since there are very less number of negative reviewed texts as compared to positive reviewed texts. For this we use a back translation technique, which essentialy converts English text to German and then back to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d3cea46a444d56bd983daa7c050798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d293e8d858d404cba5e7b6f912c49ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f70a6aad57e84fd8836425f28ab82d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/825 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b93e92c59d44901a0b7ad1b8cfd8386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b756ad08d4024e66a6852a1616d9d2f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d206ee0aecf84bdbbf7a5495c9177d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/849k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90db772ab70e46c6baf8d92426c1ea46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/849k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39bc8d7767b413eb63553f74d7cc86a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/315k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ced2a6c10bc041b7a218457bb8924128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/67.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d023b41504624c3d8396767cc189b356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/849k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af589f2ba69345fab423b075b6e3c474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/849k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993adf8ee766498b90b18fe502383ecd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/315k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "back_translation_aug = naw.BackTranslationAug(\n",
    "    from_model_name='facebook/wmt19-en-de', \n",
    "    to_model_name='facebook/wmt19-de-en'\n",
    ")\n",
    "    \n",
    "def generate_augmented_text(text):\n",
    "    augmented_text = back_translation_aug.augment(text)\n",
    "    return augmented_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  I'm really enjoying this product. Received a sample trio of the Murad products to try first, then purchased the cleanser. Great deal at $35 as a little goes a long way! I use it nightly with my Clarisonic Mia and in the morning by itself. Leaves my skin feeling clean and smooth and appears to be helping to even out my skin tone.\n",
      "\n",
      "Augmented Text:  I got a sample trio of Murad products to try first, then I bought the cleanser. A lot for $35, because a bit much works! I use it every night with my Clarisonic Mia and in the morning alone. It leaves my skin feeling clean and smooth and seems to help balance my skin tone.\n"
     ]
    }
   ],
   "source": [
    "# Example of text augmentation:\n",
    "\n",
    "text = df['review_text'][2]\n",
    "print(\"Original Text: \", text)\n",
    "print()\n",
    "print(\"Augmented Text: \", generate_augmented_text(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the augmented text basically means the same as the original text. Only the framing of the sentence is changed a bit in the augmented text. We will apply this technique to all the negatively reviewed sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/109 [00:00<?, ?it/s]/home/bikram/anaconda3/envs/alt/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "100%|██████████| 109/109 [07:07<00:00,  3.93s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    891\n",
       "0    218\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_reviews = df[df['sentiment'] == 0]['review_text'].values\n",
    "\n",
    "for text in tqdm(neg_reviews, desc=\"Augmenting text data. This might take a while...\"):\n",
    "    row = {'review_text': generate_augmented_text(text), 'sentiment': 0}\n",
    "    df = df.append(row, ignore_index=True)\n",
    "    \n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After performing text augmentation on the negatively reviewed texts, we see that the number of texts with negative(i.e. 0 labels) has doubled. This will certainly help in training the sentiment analysis model better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will simply use the data that we have processed and generated to train a sentiment analysis model. We will fine-tune a DistilBERT pre-trained model according to our usecase using the huggingface transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/augmented_data.csv', index=False)\n",
    "\n",
    "# df = pd.read_csv('data/augmented_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = df['review_text'].values.tolist()\n",
    "labels = df['sentiment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents, val_sents, train_labels, val_labels = train_test_split(reviews, labels, test_size=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1045, 2031, 2042, 2478, 2023, 2005, 2055, 1037, 3204, 1998, 1045, 2293, 2009, 1012, 2009, 2003, 2200, 11052, 9496, 6774, 1998, 26018, 3436, 2006, 2026, 5257, 3096, 1012, 1045, 2293, 2129, 2009, 5683, 1999, 1996, 2851, 2043, 1045, 5256, 2039, 1011, 2145, 4550, 1998, 2025, 3514, 2100, 1006, 2144, 1045, 2175, 3442, 2000, 1996, 9726, 2302, 12699, 2026, 2227, 1007, 1012, 1045, 2097, 3613, 2000, 2224, 2023, 2004, 2146, 2004, 2009, 7906, 2026, 3096, 3110, 3730, 1010, 11052, 28405, 1010, 1998, 4550, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
    "print(\"Text: \", train_sents[0])\n",
    "print()\n",
    "print(\"Tokenized Text: \", tokenizer([train_sents[0]], truncation=True, padding=True, max_length=128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = tokenizer(train_sents, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "val_encodings = tokenizer(val_sents, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(train_encodings),\n",
    "                            train_labels\n",
    "                            ))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "                            dict(val_encodings),\n",
    "                            val_labels\n",
    "                            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_119']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "distilbert (TFDistilBertMain multiple                  66362880  \n",
      "_________________________________________________________________\n",
      "pre_classifier (Dense)       multiple                  590592    \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  1538      \n",
      "_________________________________________________________________\n",
      "dropout_119 (Dropout)        multiple                  0         \n",
      "=================================================================\n",
      "Total params: 66,955,010\n",
      "Trainable params: 66,955,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english', num_labels=2)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "56/56 [==============================] - ETA: 0s - loss: 0.4335 - accuracy: 0.8760WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "56/56 [==============================] - 103s 2s/step - loss: 0.4324 - accuracy: 0.8760 - val_loss: 0.2185 - val_accuracy: 0.9099\n",
      "Epoch 2/10\n",
      "56/56 [==============================] - 93s 2s/step - loss: 0.1498 - accuracy: 0.9442 - val_loss: 0.1693 - val_accuracy: 0.9324\n",
      "Epoch 3/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.1141 - accuracy: 0.9643 - val_loss: 0.1685 - val_accuracy: 0.9324\n",
      "Epoch 4/10\n",
      "56/56 [==============================] - 91s 2s/step - loss: 0.0666 - accuracy: 0.9754 - val_loss: 0.1492 - val_accuracy: 0.9414\n",
      "Epoch 5/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0536 - accuracy: 0.9824 - val_loss: 0.1573 - val_accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0337 - accuracy: 0.9908 - val_loss: 0.1761 - val_accuracy: 0.9505\n",
      "Epoch 7/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0156 - accuracy: 0.9930 - val_loss: 0.1456 - val_accuracy: 0.9414\n",
      "Epoch 8/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.1781 - val_accuracy: 0.9459\n",
      "Epoch 9/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0221 - accuracy: 0.9934 - val_loss: 0.1467 - val_accuracy: 0.9505\n",
      "Epoch 10/10\n",
      "56/56 [==============================] - 92s 2s/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.1402 - val_accuracy: 0.9505\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f13541bc6d0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5, epsilon=1e-07)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset.shuffle(100).batch(16),\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_data=val_dataset.shuffle(100).batch(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at models/sent_analyzer were not used when initializing TFDistilBertForSequenceClassification: ['dropout_119']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at models/sent_analyzer and are newly initialized: ['dropout_139']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('models/sent_analyzer')\n",
    "loaded_model = TFDistilBertForSequenceClassification.from_pretrained('models/sent_analyzer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting on a sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence = \"I have dry skin and I got a sample of this product, I tried it, but it didn't work. I really wanted to love it because I heard a lot of good things about it, but unfortunately it wasn't for me.\"\n",
    "predict_input = tokenizer.encode(test_sentence,\n",
    "                                 truncation=True,\n",
    "                                 padding=True,\n",
    "                                 return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT:  I have dry skin and I got a sample of this product, I tried it, but it didn't work. I really wanted to love it because I heard a lot of good things about it, but unfortunately it wasn't for me.\n",
      "\n",
      "SENTIMENT:  Negative\n"
     ]
    }
   ],
   "source": [
    "tf_output = loaded_model.predict(predict_input)[0]\n",
    "tf_prediction = tf.nn.softmax(tf_output, axis=1)\n",
    "labels = ['Negative', 'Positive']\n",
    "label = tf.argmax(tf_prediction, axis=1)\n",
    "label = label.numpy()\n",
    "print(\"TEXT: \", test_sentence)\n",
    "print()\n",
    "print(\"SENTIMENT: \", labels[label[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alt",
   "language": "python",
   "name": "alt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
